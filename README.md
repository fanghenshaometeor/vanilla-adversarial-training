# vanilla-adversarial-training

This repo provides the PyTorch code for both **vanilla** training and **adversarial** training deep neural networks, including
- **CIFAR10 + vgg11/vgg13/vgg16/vgg19**
- **CIFAR100 + wideresnet28x5/wideresnet28x10**

## File Descriptions

`train.py,.sh` : training python and shell scripts

`attack.py,.sh` : attack python and shell scripts

`attackers.py` : attack functions

`utils.py` : utility functions

`model/` : model definitions directory

## Results

Complete results can be found in this sheet.


<!-- ## Usage

### attack

We **provide trained models** in `save` folder, including vanilla and adversarial training vgg/resnet on CIFAR10 and [modelA](https://github.com/aaron-xichen/pytorch-playground/blob/master/stl10/model.py) on STL10.
Users can directly run the `attack.sh` shell script on command line to check the robustness of these models.
The results should be similar with the values in the 5 tables above.
In addition, users can manually change the attack parameters in the `attackers.py` python script for more results under different settings.
```
$ sh attack.sh
```
- `model` : Please specify the target model network architecture.
- `model_path` : Please specify the target model path.
- `dataset` & `data_dir` : Please specify the dataset name and path.
- `gpu_id` : GPU device index.

### training

To reproduce the provided model, users can run the `train.sh` shell scripts on command line.
```
$ sh train.sh
```
- `model` : Please specify the target model network architecture. `vgg16`, `resnet18` or `aaron` are optional.
- `dataset` & `data_dir` : Please specify the dataset name and path.
- `model_dir` : Please specify where to save the trained model.
- `gpu_id` : GPU device index.
- `adv_train` : Please specify whether to use adversarial training. `True` or `False`. -->

**ATTENTION** 
- The **mean-var normalization** preprocess is **removed** in both vanilla-training and adversarial-training to keep the image pixel range [0,1].
- The adversarial training is **PGD-based**, i.e., the adversarial examples in training are generated by PGD attack. The $l_\infty$ bound is **8/255(0.031)**.
- In adversarial training, the network prameters are **updated twice** in each iteration, i.e., one normal updation on the clean samples followed by the other updation on the adversarial examples.


## Dependencies
- python 3.6
- PyTorch 1.7.1

## 

If u find the codes useful, welcome to fork and star this repo :)